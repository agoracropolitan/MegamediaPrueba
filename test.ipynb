{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6b80a0",
   "metadata": {},
   "source": [
    "<h1>3. Configuración del pipeline ETL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3bbc04",
   "metadata": {},
   "source": [
    "<h2>Paso 1: Extract - Recuperar datos desde la API de YouTube</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6504ca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'viewCount': '1600710185', 'likeCount': '18044616', 'favoriteCount': '0', 'commentCount': '2372643'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "# Clave API obtenida de Google Cloud Console\n",
    "API_KEY = ''\n",
    "\n",
    "# Crear un cliente de la API de YouTube\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Función para obtener estadísticas de un video\n",
    "def get_video_stats(video_id):\n",
    "    # Realizar la solicitud a la API de YouTube\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics',  # Especificamos que queremos las estadísticas\n",
    "        id=video_id  # ID del video de YouTube\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    # Extraer datos relevantes\n",
    "    video_stats = response['items'][0]['statistics']\n",
    "    return video_stats\n",
    "\n",
    "# Ejemplo de ID de video\n",
    "video_id = 'dQw4w9WgXcQ'  # Reemplazar con el ID del video que deseas analizar\n",
    "stats = get_video_stats(video_id)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80919c90",
   "metadata": {},
   "source": [
    "<h2>Paso 2: Transform - Limpiar y transformar los datos para adaptarlos a una base de datos time-series</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbabe10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      video_id  view_count like_count  dislike_count comment_count  \\\n",
      "0  dQw4w9WgXcQ  1600710185   18044616              0       2372643   \n",
      "\n",
      "                   timestamp  \n",
      "0 2024-12-10 03:28:14.999272  \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Función para transformar los datos\n",
    "def transform_data(video_id, stats):\n",
    "    # Transformar los datos a formato adecuado\n",
    "    transformed_data = {\n",
    "        'video_id': video_id,\n",
    "        'view_count': stats['viewCount'],\n",
    "        'like_count': stats.get('likeCount', 0),  # Algunas estadísticas pueden no estar disponibles\n",
    "        'dislike_count': stats.get('dislikeCount', 0),\n",
    "        'comment_count': stats.get('commentCount', 0),\n",
    "        'timestamp': datetime.utcnow()  # Usamos la fecha y hora actual como timestamp\n",
    "    }\n",
    "    \n",
    "    # Crear un DataFrame de pandas\n",
    "    df = pd.DataFrame([transformed_data])\n",
    "    return df\n",
    "\n",
    "# Transformar los datos del video\n",
    "df_transformed = transform_data(video_id, stats)\n",
    "print(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52f294",
   "metadata": {},
   "source": [
    "<h2>Paso 3: Load - Cargar los datos en PostgreSQL</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "CREATE TABLE youtube_video_stats (\n",
    "    video_id TEXT PRIMARY KEY,\n",
    "    view_count INT,\n",
    "    like_count INT,\n",
    "    dislike_count INT,\n",
    "    comment_count INT,\n",
    "    timestamp TIMESTAMPTZ\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09cd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configura la conexión a PostgreSQL (asegúrate de usar tus credenciales correctas)\n",
    "DB_URI = 'postgresql+psycopg2://usuario:contraseña@localhost:5432/mi_base_de_datos'\n",
    "engine = create_engine(DB_URI)\n",
    "\n",
    "# Cargar el DataFrame a la base de datos PostgreSQL\n",
    "df_transformed.to_sql('youtube_video_stats', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Datos cargados exitosamente en PostgreSQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de09e8a",
   "metadata": {},
   "source": [
    "<h1>4. Generar un notebook de análisis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e894361",
   "metadata": {},
   "source": [
    "<h2>Paso 1: Conectar a PostgreSQL y leer los datos:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1317e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leer los datos desde la base de datos PostgreSQL\n",
    "df = pd.read_sql('SELECT * FROM youtube_video_stats', engine)\n",
    "\n",
    "# Ver las primeras filas de los datos\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eace92",
   "metadata": {},
   "source": [
    "<h2>Paso 2: Análisis y visualización de los datos:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdfdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis simple de las estadísticas de los videos\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])  # Asegurarse de que el timestamp esté en formato datetime\n",
    "\n",
    "# Visualización de las visualizaciones a lo largo del tiempo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['timestamp'], df['view_count'], label='View Count')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Visualizaciones')\n",
    "plt.title('Visualizaciones de Videos a lo largo del Tiempo')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Para lograr un proceso de ETL (Extract, Transform, Load) utilizando Python y la API de YouTube Data para recuperar las reproducciones de videos y cargar esos datos en una base de datos PostgreSQL en formato de series temporales, vamos a seguir los siguientes pasos detallados.\n",
    "\n",
    "Requisitos:\n",
    "Python 3.x\n",
    "Bibliotecas necesarias: google-api-python-client, pandas, psycopg2, sqlalchemy, requests.\n",
    "Acceso a la API de YouTube Data (requiere una clave API).\n",
    "PostgreSQL en funcionamiento.\n",
    "1. Instalación de dependencias\n",
    "Antes de empezar, necesitamos instalar las bibliotecas necesarias en Python:\n",
    "\n",
    "bash\n",
    "Copiar código\n",
    "pip install google-api-python-client pandas psycopg2 sqlalchemy\n",
    "2. Obtener una clave de API de YouTube\n",
    "Para acceder a la YouTube Data API, debes crear un proyecto en Google Cloud Console, habilitar la API de YouTube Data y obtener una clave API.\n",
    "\n",
    "3. Configuración del pipeline ETL\n",
    "Paso 1: Extract - Recuperar datos desde la API de YouTube\n",
    "Aquí vamos a utilizar la biblioteca oficial google-api-python-client para obtener los datos de la API de YouTube.\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "# Clave API obtenida de Google Cloud Console\n",
    "API_KEY = 'TU_CLAVE_API'\n",
    "\n",
    "# Crear un cliente de la API de YouTube\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# Función para obtener estadísticas de un video\n",
    "def get_video_stats(video_id):\n",
    "    # Realizar la solicitud a la API de YouTube\n",
    "    request = youtube.videos().list(\n",
    "        part='statistics',  # Especificamos que queremos las estadísticas\n",
    "        id=video_id  # ID del video de YouTube\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    # Extraer datos relevantes\n",
    "    video_stats = response['items'][0]['statistics']\n",
    "    return video_stats\n",
    "\n",
    "# Ejemplo de ID de video\n",
    "video_id = 'dQw4w9WgXcQ'  # Reemplazar con el ID del video que deseas analizar\n",
    "stats = get_video_stats(video_id)\n",
    "print(stats)\n",
    "Este código obtiene las estadísticas del video especificado, incluyendo las visualizaciones (viewCount), lo cual es esencial para análisis en un ETL.\n",
    "\n",
    "Paso 2: Transform - Limpiar y transformar los datos para adaptarlos a una base de datos time-series\n",
    "En este paso, vamos a transformar los datos para que puedan ser cargados en una base de datos PostgreSQL. Los datos de YouTube estarán en un formato JSON, pero necesitamos transformarlos a un formato que se pueda almacenar en una tabla de series temporales.\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "from datetime import datetime\n",
    "\n",
    "# Función para transformar los datos\n",
    "def transform_data(video_id, stats):\n",
    "    # Transformar los datos a formato adecuado\n",
    "    transformed_data = {\n",
    "        'video_id': video_id,\n",
    "        'view_count': stats['viewCount'],\n",
    "        'like_count': stats.get('likeCount', 0),  # Algunas estadísticas pueden no estar disponibles\n",
    "        'dislike_count': stats.get('dislikeCount', 0),\n",
    "        'comment_count': stats.get('commentCount', 0),\n",
    "        'timestamp': datetime.utcnow()  # Usamos la fecha y hora actual como timestamp\n",
    "    }\n",
    "    \n",
    "    # Crear un DataFrame de pandas\n",
    "    df = pd.DataFrame([transformed_data])\n",
    "    return df\n",
    "\n",
    "# Transformar los datos del video\n",
    "df_transformed = transform_data(video_id, stats)\n",
    "print(df_transformed)\n",
    "En este paso, hemos transformado los datos de YouTube para que estén en un formato adecuado, que incluye un timestamp para que puedan ser cargados en una base de datos de series temporales.\n",
    "\n",
    "Paso 3: Load - Cargar los datos en PostgreSQL\n",
    "Ahora vamos a cargar estos datos transformados en una base de datos PostgreSQL. Para ello, necesitamos configurar una conexión con PostgreSQL usando psycopg2 y SQLAlchemy.\n",
    "\n",
    "Primero, asegúrate de tener un servidor PostgreSQL corriendo y una base de datos creada. La estructura básica para almacenar los datos en una tabla de series temporales podría ser algo como esto:\n",
    "\n",
    "sql\n",
    "Copiar código\n",
    "CREATE TABLE youtube_video_stats (\n",
    "    video_id TEXT PRIMARY KEY,\n",
    "    view_count INT,\n",
    "    like_count INT,\n",
    "    dislike_count INT,\n",
    "    comment_count INT,\n",
    "    timestamp TIMESTAMPTZ\n",
    ");\n",
    "Ahora, en Python, configuramos la conexión y cargamos los datos:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configura la conexión a PostgreSQL (asegúrate de usar tus credenciales correctas)\n",
    "DB_URI = 'postgresql+psycopg2://usuario:contraseña@localhost:5432/mi_base_de_datos'\n",
    "engine = create_engine(DB_URI)\n",
    "\n",
    "# Cargar el DataFrame a la base de datos PostgreSQL\n",
    "df_transformed.to_sql('youtube_video_stats', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Datos cargados exitosamente en PostgreSQL.\")\n",
    "Este código carga los datos transformados en la tabla youtube_video_stats en la base de datos de PostgreSQL. Utilizamos if_exists='append' para agregar nuevos registros sin sobrescribir los existentes.\n",
    "\n",
    "4. Generar un notebook de análisis\n",
    "Ahora, para generar un notebook de análisis que pueda visualizar y analizar los datos cargados, podemos utilizar Jupyter junto con pandas y matplotlib.\n",
    "\n",
    "Paso 1: Conectar a PostgreSQL y leer los datos:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leer los datos desde la base de datos PostgreSQL\n",
    "df = pd.read_sql('SELECT * FROM youtube_video_stats', engine)\n",
    "\n",
    "# Ver las primeras filas de los datos\n",
    "print(df.head())\n",
    "Paso 2: Análisis y visualización de los datos:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "# Análisis simple de las estadísticas de los videos\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])  # Asegurarse de que el timestamp esté en formato datetime\n",
    "\n",
    "# Visualización de las visualizaciones a lo largo del tiempo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['timestamp'], df['view_count'], label='View Count')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Visualizaciones')\n",
    "plt.title('Visualizaciones de Videos a lo largo del Tiempo')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "Este paso crea una visualización simple que muestra cómo las visualizaciones de los videos han cambiado a lo largo del tiempo.\n",
    "\n",
    "Resumen del pipeline ETL\n",
    "Extract: Usamos la API de YouTube para obtener las estadísticas de los videos.\n",
    "Transform: Limpiamos y transformamos los datos para adaptarlos a una estructura de base de datos time-series, añadiendo un timestamp.\n",
    "Load: Cargamos los datos transformados en PostgreSQL en una tabla de series temporales.\n",
    "Análisis y Visualización: Usamos Jupyter Notebook para analizar y visualizar los datos almacenados.\n",
    "Este proceso es completamente automatizable, lo que permite una extracción continua y un análisis de los videos de YouTube a lo largo del tiempo.\n",
    "\n",
    "Mejoras posibles\n",
    "Programación de tareas: Puedes usar herramientas como Airflow o Cron para programar la ejecución automática del ETL.\n",
    "Manejo de errores: Implementar control de errores y notificaciones en caso de fallos.\n",
    "Optimización: Si manejas grandes volúmenes de datos, puedes optimizar las consultas a la API de YouTube y el manejo de la base de datos.\n",
    "Este flujo ETL te permitirá integrar datos de YouTube de manera continua en un entorno de análisis de series temporales en PostgreSQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
